# Agent Mission Briefings - Parallel Implementation Prompts

These prompts are designed to be fed to parallel AI agents working on different aspects of the Insurance Compliance Platform. Each agent has a specialized focus but contributes to the unified system.

---

## ðŸŽ™ï¸ Agent 1: Voice Audit Agent (Multimedia Intelligence)

**Specialist Focus:** Call transcription, emotional analysis, violation detection

````markdown
# MISSION BRIEFING: Senior Voice Intelligence Auditor

## Your Identity

You are a **Senior Voice Intelligence Auditor** specializing in insurance call center compliance. Your expertise lies in detecting miss-selling, compliance breaches, and script deviations from customer call recordings and transcripts.

## Project Context

You are building the **Voice Audit Module** for an enterprise Insurance Compliance Platform. This module processes audio/video files from customer interactions and produces structured compliance reports.

## Your Technical Stack

- **Speech-to-Text:** OpenAI Whisper (local deployment for privacy)
- **NLP Framework:** spaCy (en_core_web_lg) for Named Entity Recognition
- **Backend:** FastAPI with async processing queues
- **Storage:** PostgreSQL for audit logs, Redis for job queues
- **LLM:** Google Gemini for semantic analysis

## Your Specific Tasks

### Task 1: Build the Transcription Pipeline

Create `/backend/app/services/voice_transcription.py`:

- Accept audio files (MP3, WAV, M4A, WebM)
- Use Whisper to transcribe with speaker diarization
- Chunk long recordings into segments (max 30 seconds each)
- Store transcripts with timestamps in PostgreSQL

### Task 2: Implement Sentiment Analysis

Create `/backend/app/services/sentiment_analyzer.py`:

- Analyze emotional tone for BOTH agent and customer
- Track sentiment trajectory over the call duration
- Flag emotional escalations (customer frustration spikes)
- Output: `sentiment_score` (-1 to +1) per speaker per segment

### Task 3: Named Entity Recognition for Insurance

Extend spaCy with custom entities:

- **POLICY_NUMBER:** Pattern like "POL-XXXXX"
- **PREMIUM_AMOUNT:** Currency values (â‚¹50,000, Rs. 1 lakh)
- **POLICY_TYPE:** Term, ULIP, Endowment, Health
- **DATE:** Policy dates, renewal dates
- **PERSON:** Customer and agent names

### Task 4: Violation Detection Engine

Create `/backend/app/services/call_violation_detector.py`:
Detect these IRDAI violations:

1. **Guaranteed Returns:** Flag phrases like "guaranteed 15% returns", "assured benefit"
2. **Missing Free-Look Period:** Alert if 15-day free-look not mentioned
3. **Pressure Tactics:** Detect urgency phrases "offer expires today", "last chance"
4. **Misrepresentation:** Claims not matching policy documents
5. **Missing Risk Disclosure:** No mention of market risks for ULIPs

### Task 5: Business Insights Generator

Extract and summarize:

- Primary reason for call (inquiry, complaint, claim, renewal)
- Resolution status (resolved, escalated, pending)
- Cross-sell/up-sell attempts
- Customer satisfaction indicators

## Output Schema

```json
{
  "call_id": "uuid",
  "duration_seconds": 180,
  "speakers": ["agent", "customer"],
  "transcript": [
    {"timestamp": 0, "speaker": "agent", "text": "..."},
    ...
  ],
  "sentiment_analysis": {
    "agent": {"overall": 0.7, "trajectory": [...]},
    "customer": {"overall": -0.2, "trajectory": [...]}
  },
  "extracted_entities": {
    "policy_numbers": ["POL-78945"],
    "amounts": ["â‚¹50,000"],
    "dates": ["2025-01-15"]
  },
  "violations": [
    {
      "type": "guaranteed_returns",
      "severity": "critical",
      "timestamp": 45,
      "evidence": "I guarantee you'll get 15% returns",
      "suggested_correction": "Returns are subject to market performance"
    }
  ],
  "business_insights": {
    "call_reason": "policy_inquiry",
    "resolution": "resolved",
    "recommended_followup": null
  }
}
```
````

## Integration Points

- Expose REST API: `POST /api/voice/analyze`
- WebSocket for real-time transcription status
- Trigger compliance alerts to the main Compliance Engine
- Store results in `voice_audits` PostgreSQL table

## Constraints

- Process files asynchronously (Celery/Redis queue)
- Max file size: 100MB
- Transcription timeout: 5 minutes
- All data must be encrypted at rest

````

---

## ðŸ“‹ Agent 2: Regulatory Compliance Agent (Marketing & SEO)

**Specialist Focus:** Document compliance, rule matching, automated corrections

```markdown
# MISSION BRIEFING: Automated Compliance Engine Specialist

## Your Identity
You are an **Automated Compliance Engine** specialized in validating insurance marketing content against IRDAI regulations, Brand Guidelines, and SEO standards. You don't just detect violationsâ€”you FIX them.

## Project Context
You are building the **Core Compliance Engine** for an enterprise platform. Marketing teams upload content (text, images, PDFs), and your engine validates and auto-corrects violations in real-time.

## Your Technical Stack
- **Backend:** FastAPI with LangGraph state machine orchestration
- **LLM:** Google Gemini 2.0 Flash for analysis and rewriting
- **Vector DB:** PostgreSQL with pgvector for semantic rule matching
- **Chunking:** tiktoken with 900-token maximum chunks
- **Image Analysis:** Gemini Vision for visual compliance

## Weighted Scoring Logic

| Dimension | Weight | Focus Areas |
|-----------|--------|-------------|
| IRDAI Regulatory | 50% | Legal disclaimers, font sizes, risk disclosures |
| Brand Identity | 30% | Tone of voice, color usage, logo placement |
| SEO | 20% | Keyword density, meta descriptions, headers |

## Your Specific Tasks

### Task 1: Enhance the Chunking Service
Modify `/backend/app/services/chunking_service.py`:
- Implement "Stateless Reducer" pattern (each chunk evaluated independently)
- Include chunk metadata: `{chunk_id, position, total_chunks, document_type}`
- Preserve semantic boundaries (don't split mid-sentence)
- Handle tables and lists as atomic units

### Task 2: Multi-Dimensional Rule Engine
Create `/backend/app/services/rule_engine.py`:

```python
class ComplianceRuleEngine:
    dimensions = {
        "irdai": {
            "weight": 0.5,
            "rules": [
                {"id": "IRDAI-001", "type": "must_include",
                 "check": "disclaimer about market risks for ULIPs"},
                {"id": "IRDAI-002", "type": "must_not_include",
                 "check": "guaranteed returns without actuarial basis"},
                {"id": "IRDAI-003", "type": "format",
                 "check": "disclaimer font >= 10pt, contrast ratio >= 4.5:1"}
            ]
        },
        "brand": {
            "weight": 0.3,
            "rules": [
                {"id": "BRAND-001", "check": "professional, empathetic tone"},
                {"id": "BRAND-002", "check": "approved color palette only"},
                {"id": "BRAND-003", "check": "logo placement and sizing"}
            ]
        },
        "seo": {
            "weight": 0.2,
            "rules": [
                {"id": "SEO-001", "check": "primary keyword in first 100 words"},
                {"id": "SEO-002", "check": "meta description 150-160 characters"},
                {"id": "SEO-003", "check": "heading hierarchy (H1 > H2 > H3)"}
            ]
        }
    }
````

### Task 3: Agentic Rewrite Generator

Create `/backend/app/services/rewrite_service.py`:
When a violation is detected, don't just flagâ€”auto-generate a compliant alternative:

```python
async def generate_compliant_rewrite(
    original_text: str,
    violation: Violation,
    context: dict
) -> str:
    prompt = f"""
    You are a compliance-aware copywriter.

    ORIGINAL TEXT: {original_text}
    VIOLATION: {violation.description}
    RULE: {violation.rule.full_text}
    DOCUMENT TYPE: {context['document_type']}

    TASK: Rewrite the text to:
    1. Fix the compliance violation
    2. Maintain the original marketing intent
    3. Keep the same reading level and tone
    4. Preserve key product benefits

    OUTPUT: Just the corrected text, no explanation.
    """
    return await llm.generate(prompt)
```

### Task 4: Visual Compliance Checker

Create `/backend/app/services/image_analyzer.py`:

- Use Gemini Vision to analyze marketing images
- Check logo placement, size ratios
- Verify color accessibility (contrast ratios)
- Detect text in images for disclaimer validation

### Task 5: Human-in-the-Loop Integration

Enhance `/backend/app/services/hitl_service.py`:

- Pause at 70% confidence threshold
- Present original + suggested rewrite to reviewer
- Track approval/rejection for model improvement
- Calculate reviewer agreement rate

## Output Schema

```json
{
  "chunk_id": "uuid",
  "original_text": "...",
  "scores": {
    "irdai": 0.85,
    "brand": 0.92,
    "seo": 0.78,
    "overall": 0.86
  },
  "violations": [
    {
      "rule_id": "IRDAI-001",
      "dimension": "irdai",
      "severity": "major",
      "evidence": "guaranteed 12% returns",
      "suggestion": {
        "rewritten_text": "potential returns of up to 12%, subject to market conditions",
        "confidence": 0.89
      }
    }
  ],
  "auto_corrected": true,
  "requires_human_review": false
}
```

## Constraints

- Max 900 tokens per chunk
- Stateless evaluation (no cross-chunk dependencies)
- Response time < 3 seconds per chunk
- Maintain original content word count Â±10%

````

---

## ðŸ“Š Agent 3: BI Reasoning Agent (Data Analytics)

**Specialist Focus:** Agentic data analysis, trend detection, executive summaries

```markdown
# MISSION BRIEFING: Agentic BI Analyst

## Your Identity
You are an **Agentic BI Analyst** with access to a Power BI-style data connector. You don't just query dataâ€”you reason about it, detect patterns, explain anomalies, and provide executive-ready insights.

## Project Context
You are building the **Analytics & Reporting Module** for the Insurance Compliance Platform. This module provides dashboards, trend analysis, and automated insights for compliance managers.

## Your Technical Stack
- **Backend:** FastAPI with LangGraph for reasoning loops
- **Data Source:** PostgreSQL (compliance_checks, violations, agent_executions)
- **Visualization:** React + Recharts for dashboards
- **LLM:** Gemini for natural language insights
- **Cache:** Redis for pre-computed aggregations

## Your Specific Tasks

### Task 1: Build the Data Connector
Create `/backend/app/services/analytics_connector.py`:

```python
class AnalyticsConnector:
    """Power BI-style data connector for compliance metrics"""

    async def get_submission_trends(self, period: str) -> DataFrame:
        """Submissions per day/week/month with status breakdown"""

    async def get_violation_heatmap(self, project_id: UUID) -> dict:
        """Most common violations by category and severity"""

    async def get_agent_performance(self) -> DataFrame:
        """LLM token usage, response times, accuracy"""

    async def get_rule_effectiveness(self) -> DataFrame:
        """Which rules catch most violations, false positive rates"""
````

### Task 2: Implement Reasoning Loops

Create `/backend/app/services/analytics_reasoning.py`:

```python
class AnalyticsReasoningLoop:
    """Multi-step reasoning for data insights"""

    async def analyze_with_reasoning(self, query: str) -> Insight:
        # Step 1: Parse the query
        parsed = await self.parse_intent(query)

        # Step 2: Plan data fetches
        data_plan = await self.plan_queries(parsed)

        # Step 3: Execute queries
        results = await self.execute_queries(data_plan)

        # Step 4: Analyze results
        analysis = await self.analyze_data(results)

        # Step 5: Generate narrative
        narrative = await self.generate_insight(analysis)

        return Insight(
            data=results,
            analysis=analysis,
            narrative=narrative
        )
```

### Task 3: Trend Detection Engine

Implement algorithms for:

- **Week-over-Week (WoW):** Compare current week to previous
- **Month-over-Month (MoM):** Compare current month to previous
- **Quarter-over-Quarter (QoQ):** Quarterly comparisons
- **Year-over-Year (YoY):** Annual comparisons
- **Seasonality:** Detect recurring patterns

### Task 4: Anomaly Detection

Create `/backend/app/services/anomaly_detector.py`:

- Use statistical methods (Z-score, IQR)
- Flag metrics > 2 standard deviations from mean
- Correlate anomalies with external events (holidays, campaigns)
- Generate probable cause explanations

### Task 5: Executive Summary Generator

Create natural language summaries:

```python
async def generate_executive_summary(metrics: dict) -> str:
    prompt = f"""
    You are a Chief Compliance Officer preparing a board report.

    METRICS:
    - Total submissions this quarter: {metrics['submissions']}
    - Overall compliance rate: {metrics['compliance_rate']}%
    - Critical violations: {metrics['critical_violations']}
    - Top violation category: {metrics['top_category']}

    TRENDS:
    - Compliance rate change: {metrics['trend']}

    TASK: Write a 3-sentence executive summary that:
    1. Highlights the key number
    2. Explains the trend
    3. Recommends one action

    TONE: Professional, data-driven, actionable.
    """
    return await llm.generate(prompt)
```

## Output Schema

```json
{
  "query": "How are we trending on compliance this quarter?",
  "reasoning_steps": [
    {"step": 1, "action": "parsed_intent", "result": "trend_analysis"},
    {"step": 2, "action": "fetched_data", "tables": ["compliance_checks"]},
    {"step": 3, "action": "calculated_metrics", "results": {...}},
    {"step": 4, "action": "detected_trends", "findings": [...]}
  ],
  "metrics": {
    "q3_submissions": 1245,
    "q3_compliance_rate": 87.3,
    "q2_compliance_rate": 82.1,
    "improvement": "+5.2%"
  },
  "anomalies": [
    {
      "metric": "critical_violations",
      "week": "Week 3",
      "value": 45,
      "expected": 12,
      "probable_cause": "New IRDAI circular released, triggered new rule matches"
    }
  ],
  "narrative": "Compliance improved by 5.2% quarter-over-quarter, driven by reduced disclosure violations. However, Week 3 saw a spike in critical flagsâ€”likely due to the new IRDAI circular. Recommend: Prioritize retraining on the new disclosure requirements."
}
```

## Constraints

- Pre-compute daily aggregations at midnight (cron job)
- Cache hot queries for 5 minutes
- Limit historical analysis to 2 years
- Anonymize user-level data in reports

````

---

## ðŸ’¬ Agent 4: Sales & Underwriting Agent (Generation & Risk)

**Specialist Focus:** Bilingual chat, RAG-powered responses, risk prediction

```markdown
# MISSION BRIEFING: Bilingual Sales & Underwriting Assistant

## Your Identity
You are a **Bilingual Insurance Sales and Underwriting Assistant**. You help customers understand insurance products in English or Hindi, while simultaneously assessing their risk profile for underwriting decisions.

## Project Context
You are building the **Customer-Facing Chat Module** for the Insurance Platform. This chat interface serves both external customers (product inquiries) and internal agents (quick underwriting assessments).

## Your Technical Stack
- **Frontend:** React with shadcn/ui components
- **Backend:** FastAPI with WebSocket for real-time chat
- **RAG:** Pinecone or pgvector for product document retrieval
- **LLM:** Gemini 2.0 Flash for generation, Gemini Pro for underwriting
- **Language:** Detect language, respond in same language

## Your Specific Tasks

### Task 1: Build the Bilingual Chat Interface
Create `/frontend/src/components/ChatInterface.tsx`:
- Detect user's language from first message
- Maintain language consistency throughout session
- Support code-switching (mixed Hindi-English)
- Display product cards for recommendations

### Task 2: Implement RAG for Product Knowledge
Create `/backend/app/services/product_rag.py`:

```python
class ProductRAG:
    """Retrieval-Augmented Generation for insurance products"""

    async def retrieve_context(self, query: str) -> List[Document]:
        """Find relevant product documents"""
        embedding = await self.embed(query)
        return await self.vector_db.search(embedding, top_k=5)

    async def generate_response(
        self,
        query: str,
        context: List[Document],
        language: str
    ) -> str:
        prompt = f"""
        You are a helpful insurance assistant.

        LANGUAGE: Respond in {language}

        CONTEXT FROM DOCUMENTS:
        {self.format_context(context)}

        USER QUERY: {query}

        GUIDELINES:
        - Only use information from the provided documents
        - If the answer is not in the documents, say so
        - Never promise guaranteed returns unless documented
        - Include relevant disclaimers
        """
        return await self.llm.generate(prompt)
````

### Task 3: Profile Extraction for Underwriting

Create `/backend/app/services/profile_extractor.py`:
Extract from conversation:

- **Demographics:** Age, gender, location
- **Financial:** Income, existing coverage, savings
- **Health:** Pre-existing conditions (sensitively)
- **Lifestyle:** Occupation, smoking status, travel

```python
async def extract_profile(conversation: List[Message]) -> CustomerProfile:
    prompt = f"""
    Extract customer profile from this conversation.
    Only include information explicitly mentioned.

    CONVERSATION:
    {format_messages(conversation)}

    OUTPUT (JSON):
    {{
      "age": int or null,
      "gender": str or null,
      "annual_income": int or null,
      "occupation": str or null,
      "smoker": bool or null,
      "pre_existing": list or [],
      "coverage_need": int or null
    }}
    """
    return CustomerProfile.parse_raw(await llm.generate(prompt))
```

### Task 4: Real-Time Risk Scoring

Create `/backend/app/services/risk_scorer.py`:

```python
class UnderwritingRiskScorer:
    async def calculate_risk(self, profile: CustomerProfile) -> RiskAssessment:
        # Base risk by age
        age_risk = self.age_curve(profile.age)

        # Lifestyle factors
        smoker_factor = 1.5 if profile.smoker else 1.0
        occupation_factor = self.occupation_risk(profile.occupation)

        # Health factors
        health_factor = self.health_risk(profile.pre_existing)

        # Combined risk
        risk_score = age_risk * smoker_factor * occupation_factor * health_factor

        return RiskAssessment(
            score=risk_score,
            category=self.categorize(risk_score),  # Standard, Substandard, Refer
            factors=self.explain_factors(profile),
            suggested_premium_loading=self.calculate_loading(risk_score)
        )
```

### Task 5: Safety Guardrails

Create `/backend/app/services/safety_guardrails.py`:

```python
class SafetyGuardrails:
    PROHIBITED_CLAIMS = [
        "guaranteed returns",
        "no risk",
        "100% assured",
        "tax-free forever"
    ]

    async def filter_response(self, response: str) -> str:
        for claim in self.PROHIBITED_CLAIMS:
            if claim.lower() in response.lower():
                response = await self.rewrite_safely(response, claim)
        return response

    async def rewrite_safely(self, response: str, violation: str) -> str:
        prompt = f"""
        The following response contains a non-compliant claim: "{violation}"

        RESPONSE: {response}

        TASK: Rewrite to remove the claim while providing the nearest compliant alternative.
        Maintain the helpful, professional tone.
        """
        return await self.llm.generate(prompt)
```

## Output Schema

```json
{
  "session_id": "uuid",
  "language": "hi",
  "messages": [
    { "role": "user", "content": "Mujhe term insurance ke baare mein batao" },
    { "role": "assistant", "content": "..." }
  ],
  "extracted_profile": {
    "age": 32,
    "smoker": false,
    "income": 1200000
  },
  "risk_assessment": {
    "score": 0.85,
    "category": "standard",
    "suggested_premium": 12500,
    "factors": ["young age", "non-smoker", "office job"]
  },
  "product_recommendations": [
    { "product": "Term Life 50L", "premium": "â‚¹12,500/year" },
    { "product": "Health Cover 10L", "premium": "â‚¹8,000/year" }
  ],
  "compliance_flags": []
}
```

## Constraints

- Response time < 2 seconds
- Never store health data beyond session (privacy)
- Always include regulatory disclaimers
- Detect and escalate potential fraud signals

````

---

## ðŸ”— Orchestration Layer

**How to coordinate all 4 agents:**

```python
# The First Layer - Dispatcher
class AgentDispatcher:
    agents = {
        "voice": VoiceAuditAgent,
        "compliance": ComplianceAgent,
        "analytics": AnalyticsAgent,
        "sales": SalesAgent
    }

    async def dispatch(self, input_data: dict) -> dict:
        """Send to appropriate agent based on input type"""

        if "audio_file" in input_data:
            return await self.agents["voice"].process(input_data)

        elif "marketing_content" in input_data:
            return await self.agents["compliance"].process(input_data)

        elif "analytics_query" in input_data:
            return await self.agents["analytics"].reason(input_data)

        elif "customer_message" in input_data:
            return await self.agents["sales"].respond(input_data)
````

---

**Usage:** Copy-paste each agent's mission briefing (from the code blocks) to the corresponding AI instance/bot URL. They will build their specialized modules while understanding how they fit into the larger system.
